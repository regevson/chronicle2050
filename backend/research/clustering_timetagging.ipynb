{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661218a0-9378-4520-8609-5078ac6abb37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "661218a0-9378-4520-8609-5078ac6abb37",
    "outputId": "85a495d2-4439-4c6c-dce7-8e87ba568a1f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "from sutime import SUTime\n",
    "from typing import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb99a528-8aa2-40a2-9f6a-5fb851c4cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get -q install maven\n",
    "pom_path = pathlib.Path(importlib.util.find_spec(\"sutime\").origin).parent / \"pom.xml\"\n",
    "!mvn dependency:copy-dependencies -DoutputDirectory=./jars -f {pom_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec276edb-b95b-4c2a-aac1-6a5e0566222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_env = '../.env'\n",
    "load_dotenv(dotenv_path=path_to_env)\n",
    "DB_PASSWORD = os.environ.get('DB_PASSWORD')\n",
    "DB_DOMAIN = os.environ.get('DB_DOMAIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c0077-8268-41d3-9cc9-69d6bf3e3235",
   "metadata": {
    "id": "989c0077-8268-41d3-9cc9-69d6bf3e3235"
   },
   "source": [
    "## Time Tagging Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ef505-a023-41b8-b2db-b0d563208361",
   "metadata": {
    "id": "d84ef505-a023-41b8-b2db-b0d563208361"
   },
   "outputs": [],
   "source": [
    "sutime = SUTime(mark_time_ranges=True, include_range=True)\n",
    "current_dt = datetime.now()\n",
    "#current_dt = datetime.strptime('04-29-2023', \"%m-%d-%Y\")\n",
    "\n",
    "def discard(txt1, txt2=\"\"): # for DEBUGGING\n",
    "    #print(\"<<\", txt1, txt2, \">>\")\n",
    "    #print()\n",
    "    return None\n",
    "\n",
    "def extract_temp_info(sentence: str, timestamp: str):\n",
    "    # parse the sentence along with its parent articles publication date using SUTime and convert the output to a JSON object\n",
    "    parsed_temp_info = json.loads(json.dumps(sutime.parse(sentence, timestamp)))\n",
    "    # check if any temporal information was found\n",
    "    if len(parsed_temp_info) == 0:\n",
    "        return discard(parsed_temp_info, \"no time expressions found\")\n",
    "    return parsed_temp_info\n",
    "\n",
    "def convert_timex_to_datetime(timex: str):\n",
    "    timex = timex.replace('T', ' ')\n",
    "    timex = timex.replace('X', '5') # for a decade, take the middle of it -> year ...5\n",
    "\n",
    "    # replace season codes with approximate dates\n",
    "    timex = timex.replace('FA', '11-06')\n",
    "    timex = timex.replace('SU', '08-06')\n",
    "    timex = timex.replace('SP', '05-05')\n",
    "    timex = timex.replace('WI', '02-04')\n",
    "\n",
    "    # try converting using each defined date format\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d',\n",
    "        '%Y-%m',\n",
    "        '%Y-W%W-%w',\n",
    "        '%Y-W%W',\n",
    "        '%Y',\n",
    "        '%Y-%j',\n",
    "    ]\n",
    "    for date_format in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(timex, date_format)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    # if conversion failed for all formats\n",
    "    return discard(timex, \"could not convert to datetime\")\n",
    "\n",
    "\n",
    "def extract_datetime(temporal_info: Dict):\n",
    "    days_of_week = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
    "    text_info = temporal_info.get('text', \"\")\n",
    "\n",
    "    if text_info.lower() in days_of_week: # references to weekdays are discarded bc. SUTime can't handle them well\n",
    "        return discard(temporal_info, \"reference to weekday\")\n",
    "\n",
    "    temporal_type = temporal_info.get('type', \"\")\n",
    "    timex_value = temporal_info.get('timex-value', \"\")\n",
    "\n",
    "    if timex_value:\n",
    "        if temporal_type == \"DATE\":\n",
    "            parsed_datetime = convert_timex_to_datetime(timex_value)\n",
    "            if not parsed_datetime:\n",
    "                return discard(temporal_info, \"timex-parsing failed\")\n",
    "            return parsed_datetime\n",
    "        else:\n",
    "            return discard(temporal_info, \"timex-type not supported\")\n",
    "    else:\n",
    "        return discard(temporal_info, \"timex detected but not extracted\")\n",
    "\n",
    "def collect_sentences_with_temp_exp(sentence_timestamp_pairs: List[Tuple[str, str]]):\n",
    "    # store sentences with their referenced maximum datetime\n",
    "    sentence_datetime_pairs = {}\n",
    "    for sentence, timestamp in sentence_timestamp_pairs:\n",
    "        temporal_info_list = extract_temp_info(sentence, timestamp) # sentence can contain multiple TIMEXes\n",
    "\n",
    "        if not temporal_info_list: # if no temporal information is found, skip to the next iteration\n",
    "            continue\n",
    "\n",
    "        # extract datetime values from the temporal information\n",
    "        datetime_values = [extract_datetime(temporal_info) for temporal_info in temporal_info_list]\n",
    "        # find the maximum datetime value among the extracted datetime values\n",
    "        max_datetime_value = max(filter(None, datetime_values), default=None)\n",
    "        # validate the maximum datetime value before adding it to the dictionary\n",
    "        if (not max_datetime_value or\n",
    "            max_datetime_value <= current_dt or\n",
    "            max_datetime_value.year - current_dt.year > 200):\n",
    "            continue\n",
    "\n",
    "        print(\"SUCCESS\", max_datetime_value)\n",
    "        sentence_datetime_pairs[sentence] = max_datetime_value\n",
    "\n",
    "    return sentence_datetime_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcb37e-789b-4a54-8fe6-1e346bc87141",
   "metadata": {
    "id": "c6dcb37e-789b-4a54-8fe6-1e346bc87141"
   },
   "source": [
    "## Set Up Database Connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567762e9-3d93-4b13-b6cf-1c4327875b44",
   "metadata": {
    "id": "567762e9-3d93-4b13-b6cf-1c4327875b44"
   },
   "outputs": [],
   "source": [
    "def conn_to_db(db):\n",
    "  password = quote_plus(DB_PASSWORD)\n",
    "  db_url = f\"mysql+mysqldb://root:{password}@{DB_DOMAIN}:2306/{db}\"\n",
    "  engine = create_engine(db_url)\n",
    "  return engine\n",
    "\n",
    "def download_df(db, table):\n",
    "  return pd.read_sql_table(table, conn_to_db(db))\n",
    "\n",
    "def upload_df(db, table, df):\n",
    "  df.to_sql(table, conn_to_db(db), if_exists = 'replace')\n",
    "\n",
    "def append_to_db(db, table, df):\n",
    "  df.to_sql(table, conn_to_db(db), if_exists = 'append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659fc08-a9a7-4ab7-a7fb-1f139ef89523",
   "metadata": {
    "id": "a659fc08-a9a7-4ab7-a7fb-1f139ef89523"
   },
   "source": [
    "## Set Constants For Postprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35dfe4-20a9-4365-91fa-4d9b9572020f",
   "metadata": {
    "id": "3c35dfe4-20a9-4365-91fa-4d9b9572020f"
   },
   "outputs": [],
   "source": [
    "EXTRACTED_KEYWORD_NUM = 6\n",
    "DUPLICATE_SIMILARITY_THRESHOLD = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf7ae7-5182-44d4-8306-2e95af1911d9",
   "metadata": {
    "id": "c4bf7ae7-5182-44d4-8306-2e95af1911d9"
   },
   "source": [
    "## Load Sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3104fe-00b5-4ec4-a0df-4a54d800621e",
   "metadata": {
    "id": "1c3104fe-00b5-4ec4-a0df-4a54d800621e"
   },
   "outputs": [],
   "source": [
    "query = \"elon_musk\"\n",
    "preds = download_df(\"backend\", query + \"_positives\")\n",
    "preds = preds.loc[:, [\"sentence\", \"timestamp\", \"link\", \"num_duplicates\"]] # only keep those columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca64b69-4120-4577-89ef-bf46e28bc407",
   "metadata": {
    "id": "dca64b69-4120-4577-89ef-bf46e28bc407"
   },
   "source": [
    "## Cluster With BERTopic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be78504-4fb6-4c8a-ac48-401cedccc2d7",
   "metadata": {
    "id": "5be78504-4fb6-4c8a-ac48-401cedccc2d7"
   },
   "outputs": [],
   "source": [
    "def init_bertopic():\n",
    "    sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    bertopic_model = BERTopic(\n",
    "                              embedding_model=sent_model,\n",
    "                              vectorizer_model=vectorizer_model,\n",
    "                              top_n_words=10,\n",
    "                              n_gram_range=(1,1),\n",
    "                              min_topic_size=7,\n",
    "                             )\n",
    "                              #nr_topics=100\n",
    "    return bertopic_model, sent_model\n",
    "\n",
    "def fit_bertopic(topic_model, sentence_model, sentences):\n",
    "    embeddings = sentence_model.encode(sentences)\n",
    "    topics, probabilities = topic_model.fit_transform(sentences, embeddings)\n",
    "    return topics, embeddings\n",
    "\n",
    "def show_topic_info(model):\n",
    "    freq = model.get_topic_info()\n",
    "    return freq\n",
    "\n",
    "def output_topic_contents(model, sentences, topics):\n",
    "    pd.set_option('display.max_columns', None) # show all columns\n",
    "    pd.set_option('display.width', None) # set the width of the display to be unlimited\n",
    "    pd.set_option('display.max_colwidth', None) # show full column width\n",
    "\n",
    "    corpus = sentences\n",
    "    topics_arr = np.array(topics)\n",
    "\n",
    "    for c in model.get_topic_freq()['Topic']:\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(\"Cluster:\", c)\n",
    "        print(\"Words:\", show_topic_info(model)['Name'].iloc[c])\n",
    "        print(corpus[topics_arr == c])\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b5d1f-3d5c-4a7d-80fa-5a336bc2283e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b48b5d1f-3d5c-4a7d-80fa-5a336bc2283e",
    "outputId": "5f2c0370-80e8-4c74-fec6-35aac2345a74"
   },
   "outputs": [],
   "source": [
    "bertopic_model, sentence_model = init_bertopic()\n",
    "topics, embeddings = fit_bertopic(bertopic_model, sentence_model, preds[\"sentence\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ce687-2097-49c6-8ad5-0d919ec2f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.assign(cluster_id=topics, \n",
    "                     mentions=0, \n",
    "                     embedding=embeddings.tolist(), \n",
    "                     datetime='', \n",
    "                     links=''\n",
    "                    )\n",
    "show_topic_info(bertopic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5e444-6d44-479d-b1a1-e853fa053f1f",
   "metadata": {
    "id": "31f5e444-6d44-479d-b1a1-e853fa053f1f"
   },
   "source": [
    "## Postprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34601150-d15d-4fd7-bc1d-fd53c0524115",
   "metadata": {
    "id": "34601150-d15d-4fd7-bc1d-fd53c0524115"
   },
   "outputs": [],
   "source": [
    "def extract_keywords(bertopic_model):\n",
    "    keyword_tfidf_tuples: List[Tuple[str, float]] = bertopic_model.get_topic(topic_id)\n",
    "    keywords = list(map(lambda t: t[0], keyword_tfidf_tuples))\n",
    "    keywords = [word for word in keywords if word not in stopwords.words('english')]\n",
    "    keywords = keywords[:EXTRACTED_KEYWORD_NUM]\n",
    "    topic_heading = '-'.join(keywords)\n",
    "    return topic_heading, keywords\n",
    "\n",
    "def identify_outlier_sentences(group, keywords, topic_heading):\n",
    "    indices_to_remove = []\n",
    "    for row_id, row in group.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        contained_keyword_num = sum(1 for word in keywords[:EXTRACTED_KEYWORD_NUM] if word in sentence.lower())\n",
    "\n",
    "        if contained_keyword_num < 3:\n",
    "            indices_to_remove.append(row_id)\n",
    "            continue\n",
    "\n",
    "        group.at[row_id, 'keyword_num'] = contained_keyword_num\n",
    "    return indices_to_remove\n",
    "\n",
    "def time_tag_sentences(group):\n",
    "    for row_id, row in group.iterrows():\n",
    "        sentence_datetime = collect_sentences_with_temp_exp([(row[\"sentence\"], row[\"timestamp\"])])\n",
    "        if len(sentence_datetime.values()) > 0:\n",
    "            dt = list(sentence_datetime.values())[0]\n",
    "            group.at[row_id, 'datetime'] = dt\n",
    "\n",
    "def detect_redundant_sentences(group):\n",
    "    group_embeddings = group[\"embedding\"].tolist()\n",
    "    similarity_matrix = cosine_similarity(np.array(group_embeddings))\n",
    "    similarity_matrix[similarity_matrix < DUPLICATE_SIMILARITY_THRESHOLD] = 0\n",
    "    similarity_matrix[similarity_matrix >= DUPLICATE_SIMILARITY_THRESHOLD] = 1\n",
    "    num_similars = np.sum(similarity_matrix, axis=1)\n",
    "    group['mentions'] = group['num_duplicates'] + num_similars\n",
    "\n",
    "    row_idxs, col_idxs = np.where(similarity_matrix == 1)\n",
    "    paired_indices = list(zip(row_idxs, col_idxs))\n",
    "    paired_indices = [(i, j) for i, j in paired_indices if i != j]\n",
    "\n",
    "    group.reset_index(drop=True, inplace=True)\n",
    "    proc_indeces = []\n",
    "    indices_to_remove = []\n",
    "    for i,j in paired_indices:\n",
    "        sent_i = group.loc[i, 'sentence']\n",
    "        sent_j = group.loc[j, 'sentence']\n",
    "\n",
    "        dt_i = group.loc[i, 'datetime']\n",
    "        dt_j = group.loc[j, 'datetime']\n",
    "\n",
    "        if (dt_i and not dt_j) or (not dt_i and dt_j):\n",
    "            pass\n",
    "        elif len(sent_i) >= len(sent_j):\n",
    "            indices_to_remove.append(j)\n",
    "        elif len(sent_j) > len(sent_i):\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "        if (i,j) in proc_indeces or (j,i) in proc_indeces:\n",
    "            continue\n",
    "            \n",
    "        group.at[i, 'links'] = group.loc[i, 'links'] + ',' + group.loc[j, 'link']\n",
    "        group.at[j, 'links'] = group.loc[j, 'links'] + ',' + group.loc[i, 'link']\n",
    "        proc_indeces.append((i,j))\n",
    "        \n",
    "    return indices_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e9e59-4a8e-4cfb-b421-d3408b5a159c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b9e9e59-4a8e-4cfb-b421-d3408b5a159c",
    "outputId": "d40b7987-aa06-4f59-9d99-49479a4db4bb"
   },
   "outputs": [],
   "source": [
    "# update dataframe with results (topics)\n",
    "preds_grp = preds.groupby('cluster_id') # make new dataframe (group) for every topic (cluster)\n",
    "groups = []\n",
    "topic_ids = list(set(topics))\n",
    "\n",
    "for topic_id in topic_ids:\n",
    "    group = preds_grp.get_group(topic_id)\n",
    "\n",
    "    topic_heading, keywords = extract_keywords(bertopic_model)\n",
    "    if len(keywords) < 3: # cluster-heading lacks descriptive information\n",
    "        continue\n",
    "    '''\n",
    "    '''\n",
    "    group[\"keywords\"] = topic_heading\n",
    "\n",
    "    outlier_indices = identify_outlier_sentences(group, keywords, topic_heading)\n",
    "    group.drop(outlier_indices, inplace=True)\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "\n",
    "    time_tag_sentences(group)\n",
    "\n",
    "    redundant_indices = detect_redundant_sentences(group)\n",
    "    group.drop(redundant_indices, inplace=True)\n",
    "\n",
    "    groups.append(group)\n",
    "\n",
    "preds_out = pd.concat(groups, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b708f8-b51d-4e57-9fcd-d3a553686e56",
   "metadata": {
    "id": "77b708f8-b51d-4e57-9fcd-d3a553686e56"
   },
   "source": [
    "## Upload Results to DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f45ea3-334f-46ad-b33d-acd8b30357fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "85f45ea3-334f-46ad-b33d-acd8b30357fe",
    "outputId": "3b6f0adf-beb1-4c31-dd9b-33269f2b8702",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_out = preds_out.drop(labels=['embedding', 'keyword_num', 'num_duplicates'], axis=1)\n",
    "preds_out = preds_out.sort_values(['cluster_id', 'mentions'], ascending=[True, False])\n",
    "\n",
    "print(preds_out)\n",
    "#upload_df(\"frontend\", \"topics_\" + query, preds_out)\n",
    "print(\"all done\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chronicle_env",
   "language": "python",
   "name": "chronicle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

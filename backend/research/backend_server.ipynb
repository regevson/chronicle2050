{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad43f5-39f4-4c2a-9492-d63f41b877fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "path_to_env = '../.env'\n",
    "load_dotenv(dotenv_path=path_to_env)\n",
    "DB_PASSWORD = os.environ.get('DB_PASSWORD')\n",
    "DB_DOMAIN = os.environ.get('DB_DOMAIN')\n",
    "NEWS_API_KEY = os.environ.get('NEWS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119426d-e48b-43d1-a18c-b774f949333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db-service\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def conn_to_db(db):\n",
    "  encoded = quote_plus(DB_PASSWORD)\n",
    "  db_url = f\"mysql+mysqldb://root:{encoded}@{DB_DOMAIN}:2306/{db}\"\n",
    "  engine = create_engine(db_url)\n",
    "  return engine\n",
    "\n",
    "def download_df(db, table):\n",
    "  return pd.read_sql_table(table, conn_to_db(db))\n",
    "\n",
    "def upload_to_db(db, table, df):\n",
    "    df.to_sql(table, conn_to_db(db), if_exists = 'replace')\n",
    "\n",
    "def append_to_db(db, table, df, delete):\n",
    "    df.to_sql(table, conn_to_db(db), if_exists = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f9646-031a-4377-b47d-76b47c5e4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval-service\n",
    "import requests\n",
    "\n",
    "def fetch_articles(query, from_date, to_date, num_articles):\n",
    "    all_articles = []\n",
    "    page = 1\n",
    "    per_page = 100  # max number of articles per request for Newscatcher\n",
    "    headers = {\"x-api-key\": NEWS_API_KEY}\n",
    "\n",
    "    while len(all_articles) < num_articles:\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"lang\": \"en\",\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"page\": page,\n",
    "            \"page_size\": per_page,\n",
    "            \"sort_by\": \"relevancy\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(\"https://api.newscatcherapi.com/v2/search\", params=params, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to get data: {response.content}\")\n",
    "            break\n",
    "\n",
    "        articles = response.json().get('articles', [])\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "        if len(articles) < per_page:\n",
    "            break  # No more articles available\n",
    "\n",
    "        page += 1\n",
    "        \n",
    "    articles = pd.DataFrame(all_articles)\n",
    "\n",
    "    columns_to_keep = ['title', 'excerpt', 'published_date', 'topic', 'link']\n",
    "    articles = articles.loc[:, columns_to_keep]\n",
    "    \n",
    "    articles.dropna(subset=['excerpt'], inplace=True)\n",
    "    articles = articles[articles['excerpt'].str.strip() != '']\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98684a2e-f4b7-42a3-9f12-53740b7106e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing-service\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')  # download the sentence tokenizer\n",
    "\n",
    "def split_into_sentences(articles):\n",
    "    sentences = []\n",
    "    timestamps = []\n",
    "    links = []\n",
    "    for idx, row in articles.iterrows():\n",
    "        article = row['excerpt']\n",
    "        timestamp = row['published_date']\n",
    "        link = row['link']\n",
    "        # tokenize content into sentences\n",
    "        for sentence in sent_tokenize(article):\n",
    "            sentences.append(sentence)\n",
    "            timestamps.append(timestamp)\n",
    "            links.append(link)\n",
    "\n",
    "    # create new dataframe with each sentence and its corresponding timestamp\n",
    "    sentences_df = pd.DataFrame({\n",
    "        'sentence': sentences,\n",
    "        'timestamp': timestamps,\n",
    "        'link': links\n",
    "    })\n",
    "    sentences_df.dropna(subset=['sentence'], inplace=True)\n",
    "    return sentences_df\n",
    "\n",
    "def analyze_sentence_length(sentences):\n",
    "    sentences['len'] = sentences['sentence'].apply(len)\n",
    "    return sentences[sentences['len'] >= 30]\n",
    "    \n",
    "def drop_duplicate_links(articles):\n",
    "    #print(articles.duplicated(subset=['link'], keep=False).sum())\n",
    "    articles.drop_duplicates(subset='link', keep='first', inplace=True) # duplicate articles from the same source are useless\n",
    "\n",
    "def drop_duplicates(sentences):\n",
    "    sentences['num_duplicates'] = sentences.groupby('sentence')['sentence'].transform('count')\n",
    "    return sentences.drop_duplicates(subset='sentence', keep='first')\n",
    "\n",
    "'''\n",
    "def drop_duplicates(sentences):\n",
    "    sentences['num_duplicates'] = sentences.groupby('sentence')['sentence'].transform('count')\n",
    "    aggregated_sources = sentences.groupby('sentence')['link'].agg(list).reset_index()\n",
    "    aggregated_sources.rename(columns={'link': 'links'}, inplace=True)\n",
    "    sentences.drop_duplicates(subset='sentence', keep='first', inplace=True)\n",
    "    sentences = pd.merge(sentences, aggregated_sources, on='sentence')\n",
    "    sentences['links'] = sentences['links'].apply(lambda x: ','.join(x))\n",
    "    return sentences\n",
    "'''\n",
    "\n",
    "def resolve_newline(sentences):\n",
    "    def resolve(sentence):\n",
    "        return sentence.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    sentences['sentence'] = sentences['sentence'].apply(resolve)\n",
    "\n",
    "def preprocess_news(articles):\n",
    "    drop_duplicate_links(articles)\n",
    "    sentences = split_into_sentences(articles)\n",
    "    sentences = analyze_sentence_length(sentences)\n",
    "    sentences = drop_duplicates(sentences)\n",
    "    resolve_newline(sentences)\n",
    "    return sentences\n",
    "\n",
    "def prepare_data_for_frontend(sentences):\n",
    "    stats = {'numSentsWithDate': sentences[sentences['datetime'] == ''].shape[0],\n",
    "             'numSentsWithoutDate': sentences[sentences['datetime'] != ''].shape[0],\n",
    "             'numClusters': len(sentences['cluster_id'].unique())}\n",
    "    topic_groups = sentences.groupby('cluster_id') # make new dataframe (group) for every topic (cluster)\n",
    "    topics = []\n",
    "    for cluster_id, topic_group in topic_groups:\n",
    "        json_obj = {'clusterID': cluster_id, 'keywords': topic_group.iloc[0]['keywords']}\n",
    "        avg_mentions = topic_group['mentions'].sum()/len(topic_group)\n",
    "        json_obj['avgTotalMentions'] = float(avg_mentions)\n",
    "\n",
    "        date_groups = topic_group.groupby('datetime')\n",
    "        content_with_date = []\n",
    "        content_without_date = []\n",
    "        for date, date_group in date_groups:\n",
    "            content = {}\n",
    "\n",
    "            sents = []\n",
    "            for row_id, row in date_group.iterrows():\n",
    "                sent = {'sentence': row['sentence'],\n",
    "                        'mentions': row['mentions'],\n",
    "                        'link': row['link'],\n",
    "                        'links': row['links'],\n",
    "                        'timestamp': row['timestamp'],\n",
    "                       }\n",
    "                sents.append(sent)\n",
    "\n",
    "            sum_of_mentions = date_group['mentions'].sum()\n",
    "            #sum_of_mentions = len(date_group)\n",
    "            content['sumOfMentions'] = float(sum_of_mentions)\n",
    "\n",
    "            if date:\n",
    "                content['datetime'] = str(row['datetime'])\n",
    "                content_with_date.append(content)\n",
    "            else:\n",
    "                content_without_date.append(content)\n",
    "\n",
    "            content['sentences'] = sents\n",
    "\n",
    "        json_obj['contentWithDate'] = content_with_date\n",
    "        json_obj['contentWithoutDate'] = content_without_date\n",
    "        topics.append(json_obj)\n",
    "\n",
    "    return {'stats': stats, 'data': topics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0b19c-7f8e-4ace-8a1d-4134abd36e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification-service\n",
    "\n",
    "import requests\n",
    "\n",
    "def init_classification(parsed_query):\n",
    "    print(\"start classification\")\n",
    "    url = \"https://d9c8-34-124-155-12.ngrok.io/\"\n",
    "    response = requests.get(url + \"classify/\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"classification successful\")\n",
    "    else:\n",
    "        print(\"classification UNsuccessful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d25d67-41b7-4d0e-8111-203a250e5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering-service\n",
    "\n",
    "import requests\n",
    "\n",
    "def init_clustering(parsed_query):\n",
    "    print(\"start clustering\")\n",
    "    url = \"https://d9c8-34-124-155-12.ngrok.io/\"\n",
    "    response = requests.get(url + \"cluster/\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"clustering successful\")\n",
    "    else:\n",
    "        print(\"clustering UNsuccessful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3463fa-e505-4d24-abbc-c25c9048e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging-service\n",
    "\n",
    "import requests\n",
    "\n",
    "def init_tagging(parsed_query):\n",
    "    print(\"start tagging\")\n",
    "    url = \"https://91e9-34-125-70-82.ngrok.io/\"\n",
    "    response = requests.get(url + \"tag/\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"tagging successful\")\n",
    "    else:\n",
    "        print(\"tagging UNsuccessful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c9220-f421-4f38-8fbf-96668647432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# django-views\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "# init retrieval, preprocessing and upload\n",
    "query = \"Elon Musk\"\n",
    "parsed_query = query.replace(\" \", \"_\").lower()\n",
    "articles = fetch_articles(query, '2023-09-10', '2023-10-09', 4000)\n",
    "upload_to_db(\"backend\", parsed_query + \"_articles\", articles)\n",
    "articles = download_df(\"backend\", parsed_query + \"_articles\")\n",
    "sentences = preprocess_news(articles)\n",
    "upload_to_db(\"backend\", parsed_query + \"_sentences\", sentences)\n",
    "\n",
    "# init classification, clustering, postprocessing and tagging\n",
    "'''\n",
    "init_classification(parsed_query)\n",
    "init_clustering(parsed_query)\n",
    "init_tagging(parsed_query)\n",
    "'''\n",
    "\n",
    "# init frontend-preparation\n",
    "'''\n",
    "sentences = download_df(\"frontend\", \"topics_\" + parsed_query)\n",
    "data = prepare_data_for_frontend(sentences)\n",
    "print(json.dumps(data, indent=2))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronicle_env",
   "language": "python",
   "name": "chronicle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

@inproceedings{BaezaYatesSearchingTF,
  title={Searching the future},
  author={Baeza-Yates, Ricardo},
  booktitle={SIGIR Workshop MF/IR},
  volume={5},
  year={2005}
}

@inproceedings{ManiRobustTP,
    title = "Robust Temporal Processing of News",
    author = "Mani, Inderjeet  and
      Wilson, George",
    booktitle = "Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics",
    month = oct,
    year = "2000",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P00-1010",
    doi = "10.3115/1075218.1075228",
    pages = "69--76",
}

@inproceedings{KawaiChronoseeker,
  title = {{{ChronoSeeker}}: {{Search}} Engine for Future and Past Events},
  booktitle = {Proc. of the 4th ICUIMC},
  author = {Kawai, Hideki and Jatowt, Adam and Tanaka, Katsumi and Kunieda, Kazuo and Yamada, Keiji},
  date = {2010},
  series = {{{ICUIMC}} '10},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2108616.2108647},
  abstract = {In this paper, we propose an on-demand search engine called ChronoSeeker, which allows users to find past/future events based on their interest. Our goal is providing a search engine which can collect as many future/past events as possible relevant to user's query in obtaining various future scenarios considering both predictions and histories. Two technical issues are treated, (1) efficient search method for event information and (2) accurate filtering method for removing noises from search results. To search for event information effectively, our system expands a user query by some typical expressions related to event information such as year expressions, temporal modifiers and context terms. To remove noisy information, we selected five types of features for a machine learning technique to classify candidates into event information or not. Our experiment showed that filtering performance achieved an 85\% F-measure, and that query expansion can collect dozens of times more CEs than those without expansion.},
  articleno = {25},
  isbn = {978-1-60558-893-3},
  pagetotal = {10},
  keywords = {futurology,opinion analysis,vertical search}
}

@inproceedings{JatowtAnalyzingCollective,
author = {Jatowt, Adam and Kawai, Hideki and Kanazawa, Kensuke and Tanaka, Katsumi and Kunieda, Kazuo and Yamada, Keiji},
year = {2010},
month = {04},
pages = {1123-1124},
title = {Analyzing collective view of future, time-referenced events on the web},
doi = {10.1145/1772690.1772835}
}

@inproceedings{KanhabuaRanking,
  title = {Ranking Related News Predictions},
  booktitle = {Proc. of the 34th {{ACM SIGIR}}},
  author = {Kanhabua, Nattiya and Blanco, Roi and Matthews, Michael},
  date = {2011},
  series = {{{SIGIR}} '11},
  pages = {755--764},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  abstract = {We estimate that nearly one third of news articles contain references to future events. While this information can prove crucial to understanding news stories and how events will develop for a given topic, there is currently no easy way to access this information. We propose a new task to address the problem of retrieving and ranking sentences that contain mentions to future events, which we call ranking related news predictions. In this paper, we formally define this task and propose a learning to rank approach based on 4 classes of features: term similarity, entity-based similarity, topic similarity, and temporal similarity. Through extensive evaluations using a corpus consisting of 1.8 millions news articles and 6,000 manually judged relevance pairs, we show that our approach is able to retrieve a significant number of relevant predictions related to a given topic.},
  isbn = {978-1-4503-0757-4},
  pagetotal = {10},
  keywords = {future events,news predictions,sentence retrieval and ranking}
}

@inproceedings{KanazawaImproving,
  title = {Improving Retrieval of Future-Related Information in Text Collections},
  booktitle = {{{IEEE}}/{{WIC}}/{{ACM}} 2011},
  author = {Kanazawa, Kensuke and Jatowt, Adam and Tanaka, Katsumi},
  date = {2011},
  volume = {1},
  pages = {278--283},
  doi = {10.1109/WI-IAT.2011.250}
}

@inproceedings{NiComputationalExploration,
  title = {Computational Exploration to Linguistic Structures of Future: {{Classification}} and Categorization},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  author = {Ni, Aiming and Choi, Jinho D. and Shepard, Jason and Wolff, P.},
  date = {2015}
}

@inproceedings{NakajimaAutomaticExtraction,
  title = {Automatic Extraction of References to Future Events from News Articles Using Semantic and Morphological Information},
  booktitle = {Proc. of the 24th IJCAI},
  author = {Nakajima, Yoko},
  date = {2015},
  series = {{{IJCAI}}'15},
  pages = {4385--4386},
  publisher = {{AAAI Press}},
  location = {{Buenos Aires, Argentina}},
  abstract = {In my doctoral dissertation I investigate patterns appearing in sentences referring to the future. Such patterns are useful in predicting future events. I base the study on a multiple newspaper corpora. I firstly perform a preliminary study to find out that the patterns appearing in future-reference sentences often consist of disjointed elements within a sentence. Such patterns are also usually semantically and grammatically consistent, although lexically variant. Therefore, I propose a method for automatic extraction of such patterns, applying both grammatical (morphological) and semantic information to represent sentences in morphosemantic structure, and then extract frequent patterns, including those with disjointed elements. Next, I perform a series of experiments, in which I firstly train fourteen classifier versions and compare them to choose the best one. Next, I compare my method to the state-of-the-art, and verify the final performance of the method on a new dataset. I conclude that the proposed method is capable to automatically classify future-reference sentences, significantly outperforming state-of-the-art, and reaching 76\% of F-score.},
  isbn = {978-1-57735-738-4},
  pagetotal = {2}
}

@inproceedings{YarrabellyExtractingPredictive,
  title = {Extracting Predictive Statements with Their Scope from News Articles},
  booktitle = {ICWSM},
  author = {Yarrabelly, Navya and Karlapalem, K.},
  date = {2018}
}

@article{NakajimaMethod,
  author       = {Yoko Nakajima and
                  Michal Ptaszynski and
                  Hirotoshi Honma and
                  Fumito Masui},
  title        = {A Method for Extraction of Future Reference Sentences Based on Semantic
                  Role Labeling},
  journal      = {{IEICE} Trans. Inf. Syst.},
  volume       = {99-D},
  number       = {2},
  pages        = {514--524},
  year         = {2016},
  biburl       = {https://dblp.org/rec/journals/ieicet/NakajimaPHM16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{RadinskyPredictingTheNews,
  title = {Predicting the News of Tomorrow Using Patterns in Web Search Queries},
  booktitle = {IEEE/WIC/ACM 2008},
  author = {Radinsky, Kira and Davidovich, Sagie and Markovitch, Shaul},
  date = {2008},
  volume = {1},
  pages = {363--367},
  doi = {10.1109/WIIAT.2008.215}
}

@inproceedings{RadinskyMiningTheWeb,
  title = {Mining the Web to Predict Future Events},
  booktitle = {Proc. of the 6th WSDM},
  author = {Radinsky, Kira and Horvitz, Eric},
  date = {2013},
  series = {{{WSDM}} '13},
  pages = {255--264},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  abstract = {We describe and evaluate methods for learning to forecast forthcoming events of interest from a corpus containing 22 years of news stories. We consider the examples of identifying significant increases in the likelihood of disease outbreaks, deaths, and riots in advance of the occurrence of these events in the world. We provide details of methods and studies, including the automated extraction and generalization of sequences of events from news corpora and multiple web resources. We evaluate the predictive power of the approach on real-world events withheld from the system.},
  isbn = {978-1-4503-1869-3},
  pagetotal = {10},
  keywords = {future prediction,news prediction,web knowledge for future prediction}
}

@inproceedings{SwanAutomaticGeneration,
author = {Swan, Russell and Allan, James},
title = {Automatic Generation of Overview Timelines},
year = {2000},
isbn = {1581132263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/345508.345546},
doi = {10.1145/345508.345546},
abstract = {We present a statistical model of feature occurrence over time, and develop tests based on classical hypothesis testing for significance of term appearance on a given date. Using additional classical hypothesis testing we are able to combine these terms to generate “topics” as defined by the Topic Detection and Tracking study. The groupings of terms obtained can be used to automatically generate an interactive timeline displaying the major events and topics covered by the corpus. To test the validity of our technique we extracted a large number of these topics from a test corpus and had human evaluators judge how well the selected features captured the gist of the topics, and how they overlapped with a set of known topics from the corpus. The resulting topics were highly rated by evaluators who compared them to known topics.},
booktitle = {Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {49–56},
numpages = {8},
keywords = {event detection and tracking, statistical/probabilistic models, text data mining, UIs/visualization for collection overviews},
location = {Athens, Greece},
series = {SIGIR '00}
}

@inproceedings{JatowtExtractingCollective,
  title = {Extracting Collective Expectations about the Future from Large Text Collections},
  booktitle = {Proc. of the 20th CIKM},
  author = {Jatowt, Adam and Au Yeung, Ching-man},
  date = {2011-10-24},
  pages = {1259--1264},
  publisher = {{ACM}},
  location = {{Glasgow Scotland, UK}},
  urldate = {2023-03-16},
  eventtitle = {{{CIKM}} '11: {{International Conference}} on {{Information}} and {{Knowledge Management}}},
  isbn = {978-1-4503-0717-8},
  langid = {english},
  keywords = {future-related information retrieval,temporal information}
}

@inproceedings{AllanTemporalSummaries,
  author = {Allan, James and Gupta, Rahul and Khandelwal, Vikas},
  title = {Temporal Summaries of New Topics},
  year = {2001},
  isbn = {1581133316},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/383952.383954},
  doi = {10.1145/383952.383954},
  abstract = {We discuss technology to help a person monitor changes in news coverage over time.  We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built.  We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases.  We show that simple approaches are effective, but that the problem is far from solved.},
  booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {10–18},
  numpages = {9},
  keywords = {summarization, experimental design, metrics},
  location = {New Orleans, Louisiana, USA},
  series = {SIGIR '01}
}

@inproceedings{ChieuQueryBasedEvent,
  author = {Chieu, Hai Leong and Lee, Yoong Keok},
  title = {Query Based Event Extraction along a Timeline},
  year = {2004},
  isbn = {1581138814},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1008992.1009065},
  doi = {10.1145/1008992.1009065},
  abstract = {In this paper, we present a framework and a system that extracts events relevant to a query from a collection C of documents, and places such events along a timeline. Each event is represented by a sentence extracted from C, based on the assumption that "important" events are widely cited in many documents for a period of time within which these events are of interest. In our experiments, we used queries that are event types ("earthquake") and person names (e.g. "George Bush"). Evaluation was performed using G8 leader names as queries: comparison made by human evaluators between manually and system generated timelines showed that although manually generated timelines are on average more preferable, system generated timelines are sometimes judged to be better than manually constructed ones.},
  booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {425–432},
  numpages = {8},
  keywords = {event and entity tracking, text data mining, automatic summarization, timelines},
  location = {Sheffield, United Kingdom},
  series = {SIGIR '04}
}

@inproceedings{YanTimelineGeneration,
    title = "Timeline Generation through Evolutionary Trans-Temporal Summarization",
    author = "Yan, Rui  and
      Kong, Liang  and
      Huang, Congrui  and
      Wan, Xiaojun  and
      Li, Xiaoming  and
      Zhang, Yan",
    editor = "Barzilay, Regina  and
      Johnson, Mark",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1040",
    pages = "433--443",
}

@inproceedings{SteenAbstractiveTimeline,
    title = "Abstractive Timeline Summarization",
    author = "Steen, Julius  and
      Markert, Katja",
    booktitle = "Proc. of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    pages = "21--31",
    abstract = "Timeline summarization (TLS) automatically identifies key dates of major events and provides short descriptions of what happened on these dates. Previous approaches to TLS have focused on extractive methods. In contrast, we suggest an abstractive timeline summarization system. Our system is entirely unsupervised, which makes it especially suited to TLS where there are very few gold summaries available for training of supervised systems. In addition, we present the first abstractive oracle experiments for TLS. Our system outperforms extractive competitors in terms of ROUGE when the number of input documents is high and the output requires strong compression. In these cases, our oracle experiments confirm that our approach also has a higher upper bound for ROUGE scores than extractive methods. A study with human judges shows that our abstractive system also produces output that is easy to read and understand.",
}

@inproceedings{YuMultiTimeline,
  title = {Multi-{{TimeLine}} Summarization ({{MTLS}}): {{Improving}} Timeline Summarization by Generating Multiple Summaries},
  booktitle = {Proc. of the 59th ACL and 11th IJCNLP},
  author = {Yu, Yi and Jatowt, Adam and Doucet, Antoine and Sugiyama, Kazunari and Yoshikawa, Masatoshi},
  date = {2021-08},
  pages = {377--387},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  abstract = {In this paper, we address a novel task, Multiple TimeLine Summarization (MTLS), which extends the flexibility and versatility of Time-Line Summarization (TLS). Given any collection of time-stamped news articles, MTLS automatically discovers important yet different stories and generates a corresponding time-line for each story.To achieve this, we propose a novel unsupervised summarization framework based on two-stage affinity propagation. We also introduce a quantitative evaluation measure for MTLS based on previousTLS evaluation methods. Experimental results show that our MTLS framework demonstrates high effectiveness and MTLS task can give bet-ter results than TLS.}
}.

@book{GoodfellowDeepLearning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{LundChatGPT,
   title={<scp>ChatGPT</scp> and a new academic reality: <scp>Artificial Intelligence‐written</scp> research papers and the ethics of the large language models in scholarly publishing},
   volume={74},
   ISSN={2330-1643},
   url={http://dx.doi.org/10.1002/asi.24750},
   DOI={10.1002/asi.24750},
   number={5},
   journal={Journal of the Association for Information Science and Technology},
   publisher={Wiley},
   author={Lund, Brady D. and Wang, Ting and Mannuru, Nishith Reddy and Nie, Bing and Shimray, Somipam and Wang, Ziang},
   year={2023},
   month=mar, pages={570–581} 
}

@misc{OpenAIChatGPT,
  author = {OpenAI},
  title = {Introducing ChatGPT},
  year = {2023},
  url = {https://openai.com/blog/chatgpt},
  note = {Accessed: 2024-01-06}
}

@misc{VaswaniAttentionIsAll,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{LinSurvey,
      title={A Survey of Transformers},
      author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
      year={2021},
      eprint={2106.04554},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bertArchi,
      title={RUBERT: A Bilingual Roman Urdu BERT Using Cross Lingual Transfer Learning},
      author={Usama Khalid and Mirza Omer Beg and Muhammad Umair Arshad},
      year={2021},
      eprint={2102.11278},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bert,
  abstract = {We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations by jointly conditioning on both left and right
context in all layers. As a result, the pre-trained BERT representations can be
fine-tuned with just one additional output layer to create state-of-the-art
models for a wide range of tasks, such as question answering and language
inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI
accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question
answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human
performance by 2.0%.},
  added-at = {2019-02-05T23:35:51.000+0100},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  biburl = {https://www.bibsonomy.org/bibtex/210c860e3f390c6fbfd78a3b91ab9b0af/albinzehe},
  description = {[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  interhash = {a74f4c3853d3f0340e75546639134e91},
  intrahash = {10c860e3f390c6fbfd78a3b91ab9b0af},
  keywords = {bert elmo embeddings kallimachos nlp proposal-knowledge wordembeddings},
  note = {cite arxiv:1810.04805Comment: 13 pages},
  timestamp = {2020-07-28T14:17:24.000+0200},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding},
  url = {http://arxiv.org/abs/1810.04805},
  year = 2018
}

@misc{wordpiece,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bertMLM,
  author = {James Briggs},
  title = {Masked-Language Modeling With BERT},
  year = {2021},
  url = {https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c},
  note = {Accessed: 2024-01-06}
}

@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{OttScaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
    abstract = "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT{'}14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT{'}14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
}

@misc{distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
} 

@inproceedings{BucilDistil,
  author = {Buciluundefined, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  title = {Model Compression},
  year = {2006},
  isbn = {1595933395},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1150402.1150464},
  doi = {10.1145/1150402.1150464},
  abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
  booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages = {535–541},
  numpages = {7},
  keywords = {supervised learning, model compression},
  location = {Philadelphia, PA, USA},
  series = {KDD '06}
}

@misc{HintonDistilling,
      title={Distilling the Knowledge in a Neural Network},
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{BleiLDA,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent Dirichlet Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {993–1022},
numpages = {30}
}

@article{TopToVec,
  author       = {Dimo Angelov},
  title        = {Top2Vec: Distributed Representations of Topics},
  journal      = {CoRR},
  volume       = {abs/2008.09470},
  year         = {2020},
  url          = {https://arxiv.org/abs/2008.09470},
  eprinttype    = {arXiv},
  eprint       = {2008.09470},
  timestamp    = {Fri, 28 Aug 2020 12:11:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2008-09470.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{bertopic,
      title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
      author={Maarten Grootendorst},
      year={2022},
      eprint={2203.05794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sbert,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{umap,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{hdbscan,
	year = 2017,
	month = {nov},
	publisher = {{IEEE}
},
	author = {Leland McInnes and John Healy},
	title = {Accelerated Hierarchical Density Based Clustering},
	booktitle = {ICDMW2017}
}

@inproceedings{sutime,
    title = "{SUT}ime: A library for recognizing and normalizing time expressions",
    author = "Chang, Angel X.  and
      Manning, Christopher",
    booktitle = "Proc. of the 8th LREC'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    pages = "3735--3740",
    abstract = "We describe SUTIME, a temporal tagger for recognizing and normalizing temporal expressions in English text. SUTIME is available as part of the Stanford CoreNLP pipeline and can be used to annotate documents with temporal information. It is a deterministic rule-based system designed for extensibility. Testing on the TempEval-2 evaluation corpus shows that this system outperforms state-of-the-art techniques.",
}

@inproceedings{gutime,
  author       = {Inderjeet Mani},
  editor       = {Nicolas Nicolov and
                  Kalina Bontcheva and
                  Galia Angelova and
                  Ruslan Mitkov},
  title        = {Recent developments in temporal information extraction},
  booktitle    = {Recent Advances in Natural Language Processing III, Selected Papers
                  from {RANLP} 2003, Borovets, Bulgaria},
  series       = {Current Issues in Linguistic Theory {(CILT)}},
  volume       = {260},
  pages        = {45--60},
  publisher    = {John Benjamins, Amsterdam/Philadelphia},
  year         = {2003},
  timestamp    = {Tue, 22 Feb 2005 13:47:43 +0100},
  biburl       = {https://dblp.org/rec/conf/ranlp/Mani03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{heideltime,
  author       = {Jannik Str{\"{o}}tgen and
                  Michael Gertz},
  editor       = {Katrin Erk and
                  Carlo Strapparava},
  title        = {HeidelTime: High Quality Rule-Based Extraction and Normalization of
                  Temporal Expressions},
  booktitle    = {Proceedings of the 5th International Workshop on Semantic Evaluation,
                  SemEval@ACL 2010, Uppsala University, Uppsala, Sweden, July 15-16,
                  2010},
  pages        = {321--324},
  publisher    = {The Association for Computer Linguistics},
  year         = {2010},
  url          = {https://aclanthology.org/S10-1071/},
  timestamp    = {Fri, 06 Aug 2021 00:39:39 +0200},
  biburl       = {https://dblp.org/rec/conf/semeval/StrotgenG10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{trips,
  title={TRIPS and TRIOS System for TempEval-2: Extracting Temporal Information from Text},
  author={Naushad UzZaman and James F. Allen},
  booktitle={International Workshop on Semantic Evaluation},
  year={2010},
  url={https://api.semanticscholar.org/CorpusID:1213407}
}

@misc{vuejs,
  author = {Evan You},
  title = {The Progressive JavaScript Framework},
  year = {2024},
  url = {https://vuejs.org/guide/introduction.html},
  note = {Accessed: 2024-01-06}
}

@mastersthesis{KernComparing,
  author       = {Katrin A. Kern},
  title        = {Comparing Modern Front-End Frameworks},
  school       = {Johannes Kepler University Linz},
  year         = 2022,
  address      = {Linz, Austria},
  month        = mar,
  type         = {Bachelor's Thesis},
  supervisor   = {Dr. Michael Roland},
  note         = {Bachelor's Program in Computer Science},
}



//-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------







@inproceedings{jatowt2015mapping,
  title={Mapping temporal horizons: Analysis of collective future and past related attention in Twitter},
  author={Jatowt, Adam and Antoine, {\'E}milien and Kawai, Yukiko and Akiyama, Toyokazu},
  booktitle={Proceedings of the 24th international conference on World Wide Web},
  pages={484--494},
  year={2015}
}

@inproceedings{yusuke,
  title={Supporting judgment of fact trustworthiness considering temporal and sentimental aspects},
  author={Yamamoto, Yusuke and Tezuka, Taro and Jatowt, Adam and Tanaka, Katsumi},
  booktitle={Web Information Systems Engineering-WISE 2008: 9th International Conference, Auckland, New Zealand, September 1-3, 2008. Proceedings 9},
  pages={206--220},
  year={2008},
  organization={Springer}
}

























@inproceedings{supportingAnalysis,
  author = {Jatowt, Adam and Kanazawa, Kensuke and Oyama, Satoshi and Tanaka, Katsumi},
  title = {Supporting Analysis of Future-Related Information in News Archives and the Web},
  year = {2009},
  isbn = {9781605583228},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  abstract = {A lot of future-related information is available in news articles or Web pages. This information can however differ to large extent and may fluctuate over time. It is therefore difficult for users to manually compare and aggregate it, and to re-construct the most probable course of future events. In this paper we approach a problem of automatically generating summaries of future events related to queries using data obtained from news archive collections or from the Web. We propose two methods, explicit and implicit future-related information detection. The former is based on analyzing the context of future temporal expressions in documents, while the latter relies on detecting periodical patterns in historical document collections. We present a graph-based visualization of future-related information and demonstrate its usefulness through several examples.},
  booktitle = {Proc. of the 9th ACM/IEEE-CS Joint Conference on Digital Libraries},
  pages = {115–124},
  numpages = {10},
  keywords = {event prediction, temporal information analysis, future-related information retrieval},
  location = {Austin, TX, USA},
  series = {JCDL '09}
}

}



@article{numNewsArticles,
author = {Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
year = {2020},
month = {06},
pages = {},
title = {Bias-aware news analysis using matrix-based news aggregation},
volume = {21},
journal = {International Journal on Digital Libraries},
doi = {10.1007/s00799-018-0239-9}
}
